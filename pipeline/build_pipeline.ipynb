{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the ML Pipeline\n",
    "\n",
    "In this notebook we provide a template for creating a 2-step machine learning pipeline that does data prep and training. We articulate how to test, publish and schedule the pipeline.\n",
    "\n",
    "## Convert notebooks to scripts\n",
    "\n",
    "Below we run a bash script that:\n",
    "\n",
    "1. converts the data prep and training notebooks to python scripts (we strip out markdown and the output cells from the notebook)\n",
    "2. formats the scripts (using [yapf](https://github.com/google/yapf))\n",
    "3. runs linting - this is to detect errors\n",
    "\n",
    "The linting below is set to only capture errors in the code - if you would like to see warnings, etc then you can remove the `-E` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install yapf\n",
    "\n",
    "INPUTDIR=../notebooks\n",
    "OUTPUTDIR=scripts\n",
    "\n",
    "# convert the data_prep notebook to a script, format and lint\n",
    "jupyter nbconvert --to python $INPUTDIR/01-data-prep/data_prep.ipynb --output-dir $OUTPUTDIR/01-data-prep --template=ipynb_to_py.tpl\n",
    "yapf -i $OUTPUTDIR/01-data-prep/data_prep.py\n",
    "pylint -E $OUTPUTDIR/01-data-prep/data_prep.py\n",
    "\n",
    "# convert the train notebook to a script, format and lint\n",
    "jupyter nbconvert --to python $INPUTDIR/02-train/train.ipynb --output-dir $OUTPUTDIR/02-train --template=ipynb_to_py.tpl\n",
    "yapf -i $OUTPUTDIR/02-train/train.py\n",
    "pylint -E $OUTPUTDIR/02-train/train.py\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Below we provide the salient packages for building an ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Environment, RunConfiguration, Dataset, Datastore\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter, ScheduleRecurrence, Schedule\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.dnn import PyTorch, TensorFlow\n",
    "from azureml.data import TabularDataset, FileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create compute and environment for pipeline\n",
    "\n",
    "Below we create the compute to run the ML Pipeline - feel free to change the vm_size parameter below to suit your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                idle_seconds_before_scaledown=1200,\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update conda_dependencies.yml\n",
    "\n",
    "In this directory is a conda_dependencies.yml file - you should include any package dependencies there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "my_env = Environment.from_conda_specification(\"my_pipeline\", \"./conda_dependencies.yml\")\n",
    "run_config.environment = my_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep step\n",
    "\n",
    "First we define the input data and output folder. You will need to update the `DATASET_NAME` and `DATASTORE_NAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = ''\n",
    "DATASTORE_NAME = ''\n",
    "\n",
    "input_dataset = Dataset.get_by_name(ws, DATASET_NAME)\n",
    "datastore = Datastore(ws, DATASTORE_NAME)\n",
    "training_data = PipelineData(name='training_data', datastore=datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_inputs = None\n",
    "if isinstance(input_dataset, FileDataset):\n",
    "    my_inputs = [input_dataset.as_named_input(input_dataset.name).as_mount()]\n",
    "elif isinstance(input_dataset, TabularDataset):\n",
    "    my_inputs = [input_dataset.as_named_input(input_dataset.name)]\n",
    "    \n",
    "\n",
    "data_prep_step = PythonScriptStep(script_name=\"data_prep.py\",\n",
    "                                  name=\"data_prep_step\",\n",
    "                                  arguments=[\"--input_dataset\", input_dataset.name,\n",
    "                                            \"--output_folder\", training_data],\n",
    "                                  compute_target=aml_compute, \n",
    "                                  runconfig=run_config,\n",
    "                                  inputs=my_inputs,\n",
    "                                  outputs=[training_data],\n",
    "                                  source_directory=\"scripts/01-data-prep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "We run the training script in a `PythonScriptStep`. However, if you are using the scikit, tensorflow or pytorch frameworks we would recommend that you use the equivalent AzureML estimators and an `EstimatorStep` - these provide a higher level abstraction and the backend container will be set up to exploit GPU (if you DNNs). We provide the templated code below for the different estimators - just uncomment the relevant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = PipelineParameter('param1', 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(script_name=\"train.py\",\n",
    "                                  name=\"training_step\",\n",
    "                                  arguments=[\"--training_data\", training_data,\n",
    "                                            \"--param1\", param1],\n",
    "                                  compute_target=aml_compute, \n",
    "                                  runconfig=run_config,\n",
    "                                  inputs=[training_data],\n",
    "                                  outputs=[],\n",
    "                                  source_directory=\"scripts/02-train\")\n",
    "\n",
    "# If your training script uses scikit-learn then it makes sense to use the SKLearn estimator and an estimator step\n",
    "# est = SKLearn(source_directory=\"scripts/02-train\", \n",
    "#                     compute_target=compute_target, \n",
    "#                     entry_script=\"train.py\", \n",
    "#                     environment_definition=my_env)\n",
    "# If your training script uses tensorflow then it makes sense to use the tensorflow estimator and an estimator step (n.b. ensure you are using a gpu machine)\n",
    "# est = TensorFlow(source_directory=\"scripts/02-train\", \n",
    "#                 use_gpu=True,\n",
    "#                 compute_target=compute_target,                      \n",
    "#                 entry_script=\"train.py\",\n",
    "#                 environment_definition=my_env)\n",
    "# If your training script uses pytorch then it makes sense to use the pytorch estimator and an estimator step (n.b. ensure you are using a gpu machine)\n",
    "# est = PyTorch(source_directory=\"scripts/02-train\", \n",
    "#                 use_gpu=True,\n",
    "#                 compute_target=compute_target,                      \n",
    "#                 entry_script=\"train.py\",\n",
    "#                 environment_definition=my_env)\n",
    "# Now use estimator step\n",
    "# train_step = EstimatorStep(name=\"training_step\", \n",
    "#                           estimator=est, \n",
    "#                           estimator_entry_script_arguments=[\"--training_data\", training_data, \"--param1\", param1],\n",
    "#                           inputs=[training_data], \n",
    "#                           outputs=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, \n",
    "                    steps=[data_prep_step, train_step], \n",
    "                    description=\"a 2-step data prep and training pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate, Test, Publish and Schedule the pipeline\n",
    "\n",
    "First we test the pipeline works by validating it and submitting to the job service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.validate()\n",
    "pipeline.submit(experiment_name=\"pipeline-test\", regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we publish the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up a schedule for the pipeline. Here you have two options:\n",
    "\n",
    "1. Schedule the pipeline on re-occurence i.e. time schedule (e.g. minute, hourly, daily, weekly, etc)\n",
    "2. Schedule the pipeline for when additions or modifications are made to Blobs in the Datastore. By default, the Datastore container is monitored for changes. Use the path_on_datastore parameter to instead specify a path on the Datastore to monitor for changes. Note: the path_on_datastore will be under the container for the datastore, so the actual path monitored will be container/path_on_datastore. Changes made to subfolders in the container/path will not trigger the schedule. Note: Only Blob Datastores are supported.\n",
    "\n",
    "#### Option 1: Schedule on recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=2, hours=[22], minutes=[30]) # Runs every other day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, \n",
    "                           name=\"My_Schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='Schedule_Run',\n",
    "                           recurrence=recurrence,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Schedule Run\")\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Schedule on datastore change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = Schedule.create(workspace=ws, \n",
    "                           name=\"My_Schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='Schedule_Run',\n",
    "                           datastore=datastore,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"Schedule Run\")\n",
    "                          #polling_interval=5, use polling_interval to specify how often to poll for blob additions/modifications. Default value is 5 minutes.\n",
    "                          #path_on_datastore=\"file/path\") use path_on_datastore to specify a specific folder to monitor for changes.\n",
    "\n",
    "# You may want to make sure that the schedule is provisioned properly\n",
    "# before making any further changes to the schedule\n",
    "\n",
    "print(\"Created schedule with id: {}\".format(schedule.id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
