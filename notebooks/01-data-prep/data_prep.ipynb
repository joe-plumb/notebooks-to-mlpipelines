{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook should create a dataset that will be consumed by a training job. In this notebook we conform to 'best practices' such that this notebook can be integrated into an [Azure Machine Learning Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries \n",
    "\n",
    "Any libraries that are required for the data prep step should be imported in the next cell. This ensures that the created py script for our pipeline conforms to [PEP8 styling guide best practice](https://www.python.org/dev/peps/pep-0008/#imports)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tempfile\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from azureml.core import Dataset, Workspace, Datastore\n",
    "from azureml.data import TabularDataset, FileDataset\n",
    "from azureml.core.run import _OfflineRun, Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting script parameters with argparse\n",
    "\n",
    "Any parameters such as dataset name or output folder name should be defined using argparse. This is in general good practice and will also make the transition to pipelines more seamless. The default value used should be what you want to use in this notebook within the compute instance, for example if we are wanting to prepare a registered dataset called 'my_raw_data' then you would use:\n",
    "\n",
    "```\n",
    "parser.add_argument(\"--input_dataset\", default=\"my_raw_data\")\n",
    "```\n",
    "\n",
    "If you have more than one dataset than the one provided below then you can add additional arguments. Moreover, if you want to have a variable with better semantic meaning (e.g. sales_dataset) then feel free to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_dataset\",\n",
    "                    default=\"\",\n",
    "                    help=\"the input dataset name\")\n",
    "parser.add_argument(\"--output_folder\",\n",
    "                    default=\"outputs\",\n",
    "                    help=\"the folder name where you want to place outputs\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "dataset_name1 = args.input_dataset\n",
    "output_folder = args.output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data (must be a registered dataset in AzureML workspace)\n",
    "\n",
    "Below we have written a function for you that will fetch a _registered_ dataset. If you have not registered a dataset for this project then please go to the [Azure Machine Studio](https://ml.azure.com/), locate __Datasets__ (under __Assets__ in the left hand menu) and click on __Create Dataset__. Follow the instructions to register your dataset.\n",
    "\n",
    "The function below will return:\n",
    "\n",
    "* Dataset object\n",
    "* Mount Folder. If the dataset is of type File then this will be the mounted folder path to where the dataset is mounted. Otherwise, this will be returned as a None object.\n",
    "\n",
    "This function is designed to work whether you are running in a Compute Instance or whether this is used in another compute target (e.g. training cluster) as part of a AzureML pipeline. Therefore, you can have an interactive session in compute instance with this notebook to do EDA and prepare data for training but will not have to make changes to the code to incorporate into an AzureML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_registered_dataset(dataset_name):\n",
    "    run = Run.get_context()\n",
    "    dset = None\n",
    "    mount_folder = ''\n",
    "\n",
    "    if isinstance(run, _OfflineRun):\n",
    "        ws = Workspace.from_config()\n",
    "        dset = Dataset.get_by_name(ws, dataset_name)\n",
    "\n",
    "        if isinstance(dset, FileDataset):\n",
    "            mount_folder = tempfile.mkdtemp()\n",
    "            ws = Workspace.from_config()\n",
    "            dset = Dataset.get_by_name(ws, dataset_name)\n",
    "            print('This is a file dataset and therefore mounting to ' + mount_folder)\n",
    "            mount_context = dset.mount(mount_folder)\n",
    "            mount_context.start()\n",
    "    else:\n",
    "        ws = run.experiment.workspace\n",
    "        print(\"dataset name \" + dataset_name)\n",
    "        dset = run.input_datasets[dataset_name]\n",
    "        print(dset)\n",
    "        if isinstance(dset, str):\n",
    "            mount_folder = dset\n",
    "            print('This is a file dataset and therefore it has already been mounted to ' + mount_folder)\n",
    "            print('contents of folder')\n",
    "            print(os.listdir(mount_folder))\n",
    "        \n",
    "    \n",
    "    return dset, mount_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run the function by providing the dataset name provided by the argument defined by the script parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, mount_folder = get_registered_dataset(dataset_name1)\n",
    "\n",
    "# if the dataset is tabular type and you want to render it into pandas dataframe use:\n",
    "# dataframe = dataset.to_pandas_dataframe()\n",
    "\n",
    "# if the dataset is tabular and you want to render it into a spark dataframe use:\n",
    "# dataframe = dataset.to_spark_dataframe()\n",
    "\n",
    "# if you want to take (say) a 10% sample of a tabular dataset use:\n",
    "# dataframe = dataset.take_sample(probability=0.10).to_pandas_dataframe()\n",
    "\n",
    "# if you want to take the top n number of data points from a tabular dataset use:\n",
    "# dataframe = dataset.take(100).to_pandas_dataframe()\n",
    "\n",
    "# if using a file dataset then you can view the mounted files using:\n",
    "# os.listdir(mount_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data prep code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out training dataset(s) for next step in pipeline \n",
    "\n",
    "Below you should write out your training datasets to the `output_folder` parameter defined in the argparse section above. Below we create the directory for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# below you should write your training sets (or any other data required for training) into the folder created above e.g.\n",
    "# file_path = os.path.join(output_folder, 'training_data.csv')\n",
    "# pd.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
